{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Named Entity Recognition (NER) using Transformers data from CoNLL 2003 shared task\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Named Entity Recognition (NER)** is a crucial task in natural language processing (NLP) that involves identifying and categorizing named entities within a text.\n",
        "\n",
        "Named entities typically include specific types such as **\"Person,\" \"Location,\" \"Organization,\" and \"Dates,\"** among others. Essentially,\n",
        "\n",
        "NER is a token classification task where each token (word or phrase) in a sentence is classified into one or more of these predetermined categories.\n",
        "\n",
        "By tagging entities within text, NER facilitates the extraction of structured information from unstructured data, making it easier to understand and analyze.\n",
        "\n",
        "For instance, in a sentence like **\"Barack Obama was born in Honolulu,\"** NER would label **\"Barack Obama\"** as a `\"Person\"` and `\"Honolulu\"` as a `\"Location.\"`\n",
        "\n",
        "This process is fundamental in various applications, including information retrieval, question answering, and content recommendation systems."
      ],
      "metadata": {
        "id": "taaO8g6dwiLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "b_0VowLXxCBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DL5jtnj0wcMv"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade keras tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 -q install --upgrade datasets"
      ],
      "metadata": {
        "id": "cs4oPhjC5V3I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import ops\n",
        "from keras import layers\n",
        "from pathlib import Path\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "from conlleval import evaluate"
      ],
      "metadata": {
        "id": "D1zxqORwxZ_4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Transformer Block"
      ],
      "metadata": {
        "id": "shePSt8A6dZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super().__init__()\n",
        "        self.att = keras.layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [\n",
        "                keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                keras.layers.Dense(embed_dim),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.layernorm1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = keras.layers.Dropout(rate)\n",
        "        self.dropout2 = keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "L0uHWOUW05ZW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define a TokenAndPositionEmbedding Layer"
      ],
      "metadata": {
        "id": "cnTJOJlWBT7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.token_emb = keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        maxlen = ops.shape(inputs)[-1]\n",
        "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
        "        position_embeddings = self.pos_emb(positions)\n",
        "        token_embeddings = self.token_emb(inputs)\n",
        "        return token_embeddings + position_embeddings"
      ],
      "metadata": {
        "id": "6rv2A4PxBRsN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the NER model"
      ],
      "metadata": {
        "id": "RI_mws4vB4FK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NERModel(keras.Model):\n",
        "    def __init__(\n",
        "        self, num_tags, vocab_size, maxlen=128, embed_dim=32, num_heads=2, ff_dim=32\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "        self.transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "        self.dropout1 = layers.Dropout(0.1)\n",
        "        self.ff = layers.Dense(ff_dim, activation=\"relu\")\n",
        "        self.dropout2 = layers.Dropout(0.1)\n",
        "        self.ff_final = layers.Dense(num_tags, activation=\"softmax\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        x = self.transformer_block(x)\n",
        "        x = self.dropout1(x, training=training)\n",
        "        x = self.ff(x)\n",
        "        x = self.dropout2(x, training=training)\n",
        "        x = self.ff_final(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "IysL6Si1B3Bo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the CoNLL 2003 dataset"
      ],
      "metadata": {
        "id": "qAS2gK6mCoZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxU6L3C3Cjyx",
        "outputId": "0acc0db4-877d-4c17-946f-748001a4b3b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-02 18:51:57--  https://raw.githubusercontent.com/sighsmile/conlleval/master/conlleval.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7502 (7.3K) [text/plain]\n",
            "Saving to: ‘conlleval.py.4’\n",
            "\n",
            "\rconlleval.py.4        0%[                    ]       0  --.-KB/s               \rconlleval.py.4      100%[===================>]   7.33K  --.-KB/s    in 0s      \n",
            "\n",
            "2024-08-02 18:51:57 (56.5 MB/s) - ‘conlleval.py.4’ saved [7502/7502]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conll_data = load_dataset(\"conll2003\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RdidUt-C2tC",
        "outputId": "558a299e-774b-44e0-cf65-93dd7d154a6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Export this data to a tab-separated file format\n",
        "def export_to_file(export_file_path, data):\n",
        "    with open(export_file_path, \"w\") as f:\n",
        "        for record in data:\n",
        "            ner_tags = record[\"ner_tags\"]\n",
        "            tokens = record[\"tokens\"]\n",
        "            if len(tokens) > 0:\n",
        "                f.write(\n",
        "                    str(len(tokens))\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(tokens)\n",
        "                    + \"\\t\"\n",
        "                    + \"\\t\".join(map(str, ner_tags))\n",
        "                    + \"\\n\"\n",
        "                )\n",
        "\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "export_to_file(\"./data/conll_train.txt\", conll_data[\"train\"])\n",
        "export_to_file(\"./data/conll_val.txt\", conll_data[\"validation\"])"
      ],
      "metadata": {
        "id": "JR3PM8L2DIcl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make the NER label lookup table"
      ],
      "metadata": {
        "id": "NdtBttBlDqTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_tag_lookup_table():\n",
        "    iob_labels = [\"B\", \"I\"]\n",
        "    ner_labels = [\"PER\", \"ORG\", \"LOC\", \"MISC\"]\n",
        "    all_labels = [(label1, label2) for label2 in ner_labels for label1 in iob_labels]\n",
        "    all_labels = [\"-\".join([a, b]) for a, b in all_labels]\n",
        "    all_labels = [\"[PAD]\", \"O\"] + all_labels\n",
        "    return dict(zip(range(0, len(all_labels) + 1), all_labels))\n",
        "\n",
        "mapping = make_tag_lookup_table()\n",
        "print(mapping)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDehdKTlDl4B",
        "outputId": "7ff11b70-d54c-4b48-c6a6-9586e83008e8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '[PAD]', 1: 'O', 2: 'B-PER', 3: 'I-PER', 4: 'B-ORG', 5: 'I-ORG', 6: 'B-LOC', 7: 'I-LOC', 8: 'B-MISC', 9: 'I-MISC'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Get a list of all tokens in the training dataset\n",
        "all_tokens = sum(conll_data[\"train\"][\"tokens\"], [])\n",
        "all_tokens_array = np.array(list(map(str.lower, all_tokens)))\n",
        "\n",
        "counter = Counter(all_tokens_array)\n",
        "print(len(counter))\n",
        "\n",
        "num_tags = len(mapping)\n",
        "vocab_size = 20000\n",
        "\n",
        "# Take most common words (vocab_size - 2) from training data\n",
        "# 'StringLookup' class uses addintional tokens (unknown & a masking)\n",
        "vocabulary = [token for token, count in counter.most_common(vocab_size - 2)]\n",
        "\n",
        "# The Stringlook class will convert tokens to token IDs\n",
        "lookup_layer = keras.layers.StringLookup(\n",
        "    vocabulary=vocabulary\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO2U3FaWD56V",
        "outputId": "ef33a5fa-a60e-459b-c744-281bef499d23"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From training and validation data create 2 new datasets\n",
        "train_data = tf.data.TextLineDataset(\"./data/conll_train.txt\")\n",
        "val_data = tf.data.TextLineDataset(\"./data/conll_val.txt\")"
      ],
      "metadata": {
        "id": "qppJ0C0IF7KC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a line from dataset\n",
        "print(list(train_data.take(1).as_numpy_iterator()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnmmTMBsH0Yz",
        "outputId": "0dc137c7-a6f1-4620-b19a-15640a263562"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[b'9\\tEU\\trejects\\tGerman\\tcall\\tto\\tboycott\\tBritish\\tlamb\\t.\\t3\\t0\\t7\\t0\\t0\\t0\\t7\\t0\\t0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a map function to transform the data in the dataset\n",
        "def map_record_to_training_data(record):\n",
        "    record = tf.strings.split(record, sep=\"\\t\")\n",
        "    length = tf.strings.to_number(record[0], out_type=tf.int32)\n",
        "    tokens = record[1 : length + 1]\n",
        "    tags = record[length + 1 :]\n",
        "    tags = tf.strings.to_number(tags, out_type=tf.int64)\n",
        "    tags += 1\n",
        "    return tokens, tags\n",
        "\n",
        "def lowercase_and_convert_to_ids(tokens):\n",
        "    tokens = tf.strings.lower(tokens)\n",
        "    return lookup_layer(tokens)\n",
        "\n",
        "\n",
        "# As Each record has a different length, We use `padded_batch`\n",
        "batch_size = 32\n",
        "train_dataset = (\n",
        "    train_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    val_data.map(map_record_to_training_data)\n",
        "    .map(lambda x, y: (lowercase_and_convert_to_ids(x), y))\n",
        "    .padded_batch(batch_size)\n",
        ")\n",
        "\n",
        "ner_model = NERModel(num_tags, vocab_size, embed_dim=32, num_heads=4, ff_dim=64)"
      ],
      "metadata": {
        "id": "UyB94g8gIBqS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom loss function to ignore loss from padded tokens\n",
        "class CustomNonPaddingTokenLoss(keras.losses.Loss):\n",
        "    def __init__(self, name=\"custom_ner_loss\"):\n",
        "        super().__init__(name=name)\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction=None\n",
        "        )\n",
        "        loss = loss_fn(y_true, y_pred)\n",
        "        mask = ops.cast((y_true > 0), dtype=tf.float32)\n",
        "        loss = loss * mask\n",
        "        return ops.sum(loss) / ops.sum(mask)\n",
        "\n",
        "loss = CustomNonPaddingTokenLoss()"
      ],
      "metadata": {
        "id": "K3YWFVgTJGks"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compile and Fit the Model"
      ],
      "metadata": {
        "id": "MWpbuu83J0qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.run_functions_eagerly(True)\n",
        "ner_model.compile(optimizer=\"adam\", loss=loss)\n",
        "ner_model.fit(train_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxorUQZFJznR",
        "outputId": "f4d7e3bf-2314-4863-c1cf-9775899ac71a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 269ms/step - loss: 0.9082\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 252ms/step - loss: 0.2883\n",
            "Epoch 3/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 259ms/step - loss: 0.1629\n",
            "Epoch 4/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 256ms/step - loss: 0.1214\n",
            "Epoch 5/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 261ms/step - loss: 0.1004\n",
            "Epoch 6/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 260ms/step - loss: 0.0818\n",
            "Epoch 7/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 259ms/step - loss: 0.0650\n",
            "Epoch 8/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 267ms/step - loss: 0.0533\n",
            "Epoch 9/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 261ms/step - loss: 0.0463\n",
            "Epoch 10/10\n",
            "\u001b[1m439/439\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 258ms/step - loss: 0.0429\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c1442865540>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_convert_to_ids(text):\n",
        "    tokens = text.split()\n",
        "    return lowercase_and_convert_to_ids(tokens)\n",
        "\n",
        "# Sample inference using the training model\n",
        "sample_input = tokenize_and_convert_to_ids(\n",
        "    \"eu rejects german call to boycott british lamb\"\n",
        ")\n",
        "sample_input = ops.reshape(sample_input, [1, -1])\n",
        "print(sample_input)\n",
        "\n",
        "output = ner_model.predict(sample_input)\n",
        "prediction = np.argmax(output, axis=-1)[0]\n",
        "prediction = [mapping[i] for i in prediction]\n",
        "\n",
        "# eu -> B-ORG, german -> B-MISC, british -> B-MISC\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0nqX0JXZUX_",
        "outputId": "dc0157ec-f21d-4900-9d7c-bf33c9fdc7b2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[  988 10950   204   628     6  3938   215  5773]], shape=(1, 8), dtype=int64)\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics Calculation"
      ],
      "metadata": {
        "id": "KV27Pq4-PA-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(dataset):\n",
        "    all_true_tag_ids, all_predicted_tag_ids = [], []\n",
        "\n",
        "    for x, y in dataset:\n",
        "        output = ner_model.predict(x, verbose=0)\n",
        "        predictions = ops.argmax(output, axis=-1)\n",
        "        predictions = ops.reshape(predictions, [-1])\n",
        "\n",
        "        true_tag_ids = ops.reshape(y, [-1])\n",
        "\n",
        "        mask = (true_tag_ids > 0) & (predictions > 0)\n",
        "        true_tag_ids = true_tag_ids[mask]\n",
        "        predicted_tag_ids = predictions[mask]\n",
        "\n",
        "        all_true_tag_ids.append(true_tag_ids)\n",
        "        all_predicted_tag_ids.append(predicted_tag_ids)\n",
        "\n",
        "    all_true_tag_ids = np.concatenate(all_true_tag_ids)\n",
        "    all_predicted_tag_ids = np.concatenate(all_predicted_tag_ids)\n",
        "\n",
        "    predicted_tags = [mapping[tag] for tag in all_predicted_tag_ids]\n",
        "    real_tags = [mapping[tag] for tag in all_true_tag_ids]\n",
        "\n",
        "    evaluate(real_tags, predicted_tags)\n",
        "\n",
        "calculate_metrics(val_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztZtd1-QPEGr",
        "outputId": "440de699-9378-4df1-a378-11970db37111"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processed 51362 tokens with 5942 phrases; found: 5717 phrases; correct: 3994.\n",
            "accuracy:  65.15%; (non-O)\n",
            "accuracy:  93.20%; precision:  69.86%; recall:  67.22%; FB1:  68.51\n",
            "              LOC: precision:  84.90%; recall:  80.19%; FB1:  82.47  1735\n",
            "             MISC: precision:  71.76%; recall:  67.79%; FB1:  69.72  871\n",
            "              ORG: precision:  56.06%; recall:  64.13%; FB1:  59.83  1534\n",
            "              PER: precision:  65.69%; recall:  56.24%; FB1:  60.60  1577\n"
          ]
        }
      ]
    }
  ]
}